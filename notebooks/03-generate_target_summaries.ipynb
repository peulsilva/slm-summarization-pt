{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Feb 23 11:35:31 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A5000               Off |   00000000:01:00.0 Off |                  Off |\n",
      "| 42%   70C    P2            209W /  230W |    4289MiB /  24564MiB |     88%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A    495846      G   /usr/libexec/Xorg                              94MiB |\n",
      "|    0   N/A  N/A    495872      G   /usr/bin/gnome-shell                           19MiB |\n",
      "|    0   N/A  N/A   1077393      C   python                                       4162MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if 'notebooks' in os.getcwd():\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "from huggingface_hub import notebook_login\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import aiohttp\n",
    "import asyncio\n",
    "from tqdm.asyncio import tqdm\n",
    "import torch\n",
    "import scienceplots\n",
    "plt.style.use(['science', 'no-latex'])\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from src.text_utils import trim_text_to_token_limit\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_per_process_memory_fraction(0.6, device=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate target summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will generate target summaries with a larger LM.\n",
    "\n",
    "Let's start by loading everything into the machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_json(\"data/wikipedia_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>base_text</th>\n",
       "      <th>num_views</th>\n",
       "      <th>first_paragraph</th>\n",
       "      <th>predicted_topic</th>\n",
       "      <th>__index_level_0__</th>\n",
       "      <th>num_words</th>\n",
       "      <th>weight</th>\n",
       "      <th>word_weighting</th>\n",
       "      <th>num_tokens_base_text</th>\n",
       "      <th>text</th>\n",
       "      <th>num_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4629</th>\n",
       "      <td>9513</td>\n",
       "      <td>https://pt.wikipedia.org/wiki/Conhecimento</td>\n",
       "      <td>Conhecimento</td>\n",
       "      <td>Conhecimento (do latim cognoscere, \"ato de con...</td>\n",
       "      <td>129010</td>\n",
       "      <td>Conhecimento (do latim cognoscere, \"ato de con...</td>\n",
       "      <td>politica e neg√≥cios</td>\n",
       "      <td>4656</td>\n",
       "      <td>2212</td>\n",
       "      <td>285370120</td>\n",
       "      <td>True</td>\n",
       "      <td>4010</td>\n",
       "      <td>Conhecimento (do latim cognoscere, \"ato de con...</td>\n",
       "      <td>1785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>1337</td>\n",
       "      <td>https://pt.wikipedia.org/wiki/Nazismo</td>\n",
       "      <td>Nazismo</td>\n",
       "      <td>O nazismo (), oficialmente nacional-socialismo...</td>\n",
       "      <td>467240</td>\n",
       "      <td>O nazismo (), oficialmente nacional-socialismo...</td>\n",
       "      <td>politica e neg√≥cios</td>\n",
       "      <td>697</td>\n",
       "      <td>10530</td>\n",
       "      <td>4920037200</td>\n",
       "      <td>True</td>\n",
       "      <td>19394</td>\n",
       "      <td>O nazismo (), oficialmente nacional-socialismo...</td>\n",
       "      <td>2864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>1718</td>\n",
       "      <td>https://pt.wikipedia.org/wiki/Sergipe</td>\n",
       "      <td>Sergipe</td>\n",
       "      <td>Sergipe √© uma das 27 unidades federativas do B...</td>\n",
       "      <td>314672</td>\n",
       "      <td>Sergipe √© uma das 27 unidades federativas do B...</td>\n",
       "      <td>politica e neg√≥cios</td>\n",
       "      <td>908</td>\n",
       "      <td>5172</td>\n",
       "      <td>1627483584</td>\n",
       "      <td>True</td>\n",
       "      <td>9891</td>\n",
       "      <td>Sergipe √© uma das 27 unidades federativas do B...</td>\n",
       "      <td>3041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1465</th>\n",
       "      <td>3832</td>\n",
       "      <td>https://pt.wikipedia.org/wiki/Su%C3%AD%C3%A7a</td>\n",
       "      <td>Su√≠√ßa</td>\n",
       "      <td>Su√≠√ßa ( ; em su√≠√ßo-alem√£o: Schwyz ou Schwiiz ;...</td>\n",
       "      <td>588196</td>\n",
       "      <td>Su√≠√ßa ( ; em su√≠√ßo-alem√£o: Schwyz ou Schwiiz ;...</td>\n",
       "      <td>politica e neg√≥cios</td>\n",
       "      <td>1475</td>\n",
       "      <td>9452</td>\n",
       "      <td>5559628592</td>\n",
       "      <td>True</td>\n",
       "      <td>17521</td>\n",
       "      <td>Su√≠√ßa ( ; em su√≠√ßo-alem√£o: Schwyz ou Schwiiz ;...</td>\n",
       "      <td>674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>302</td>\n",
       "      <td>https://pt.wikipedia.org/wiki/Arist%C3%B3teles</td>\n",
       "      <td>Arist√≥teles</td>\n",
       "      <td>Arist√≥teles (; Estagira,  ‚Äì Atenas, ) foi um f...</td>\n",
       "      <td>348607</td>\n",
       "      <td>Arist√≥teles (; Estagira,  ‚Äì Atenas, ) foi um f...</td>\n",
       "      <td>politica e neg√≥cios</td>\n",
       "      <td>49</td>\n",
       "      <td>7075</td>\n",
       "      <td>2466394525</td>\n",
       "      <td>True</td>\n",
       "      <td>12841</td>\n",
       "      <td>Arist√≥teles (; Estagira,  ‚Äì Atenas, ) foi um f...</td>\n",
       "      <td>2666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                             url         title  \\\n",
       "4629  9513      https://pt.wikipedia.org/wiki/Conhecimento  Conhecimento   \n",
       "691   1337           https://pt.wikipedia.org/wiki/Nazismo       Nazismo   \n",
       "901   1718           https://pt.wikipedia.org/wiki/Sergipe       Sergipe   \n",
       "1465  3832   https://pt.wikipedia.org/wiki/Su%C3%AD%C3%A7a         Su√≠√ßa   \n",
       "48     302  https://pt.wikipedia.org/wiki/Arist%C3%B3teles   Arist√≥teles   \n",
       "\n",
       "                                              base_text  num_views  \\\n",
       "4629  Conhecimento (do latim cognoscere, \"ato de con...     129010   \n",
       "691   O nazismo (), oficialmente nacional-socialismo...     467240   \n",
       "901   Sergipe √© uma das 27 unidades federativas do B...     314672   \n",
       "1465  Su√≠√ßa ( ; em su√≠√ßo-alem√£o: Schwyz ou Schwiiz ;...     588196   \n",
       "48    Arist√≥teles (; Estagira,  ‚Äì Atenas, ) foi um f...     348607   \n",
       "\n",
       "                                        first_paragraph      predicted_topic  \\\n",
       "4629  Conhecimento (do latim cognoscere, \"ato de con...  politica e neg√≥cios   \n",
       "691   O nazismo (), oficialmente nacional-socialismo...  politica e neg√≥cios   \n",
       "901   Sergipe √© uma das 27 unidades federativas do B...  politica e neg√≥cios   \n",
       "1465  Su√≠√ßa ( ; em su√≠√ßo-alem√£o: Schwyz ou Schwiiz ;...  politica e neg√≥cios   \n",
       "48    Arist√≥teles (; Estagira,  ‚Äì Atenas, ) foi um f...  politica e neg√≥cios   \n",
       "\n",
       "      __index_level_0__  num_words      weight  word_weighting  \\\n",
       "4629               4656       2212   285370120            True   \n",
       "691                 697      10530  4920037200            True   \n",
       "901                 908       5172  1627483584            True   \n",
       "1465               1475       9452  5559628592            True   \n",
       "48                   49       7075  2466394525            True   \n",
       "\n",
       "      num_tokens_base_text                                               text  \\\n",
       "4629                  4010  Conhecimento (do latim cognoscere, \"ato de con...   \n",
       "691                  19394  O nazismo (), oficialmente nacional-socialismo...   \n",
       "901                   9891  Sergipe √© uma das 27 unidades federativas do B...   \n",
       "1465                 17521  Su√≠√ßa ( ; em su√≠√ßo-alem√£o: Schwyz ou Schwiiz ;...   \n",
       "48                   12841  Arist√≥teles (; Estagira,  ‚Äì Atenas, ) foi um f...   \n",
       "\n",
       "      num_tokens  \n",
       "4629        1785  \n",
       "691         2864  \n",
       "901         3041  \n",
       "1465         674  \n",
       "48          2666  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-23 11:35:44 __init__.py:207] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A5000. Max memory: 23.651 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/meta-llama-3.1-8b-instruct-bnb-4bit with actual GPU utilization = 41.12%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.6 with VRAM = 23.65 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 6000. Num Sequences = 160.\n",
      "Unsloth: vLLM's KV Cache can use up to 3.39 GB. Also swap space = 6 GB.\n",
      "INFO 02-23 11:35:53 config.py:549] This model supports multiple tasks: {'embed', 'generate', 'classify', 'score', 'reward'}. Defaulting to 'generate'.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection'], 'llm_int8_threshold': 6.0}\n",
      "INFO 02-23 11:35:53 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='unsloth/meta-llama-3.1-8b-instruct-bnb-4bit', speculative_config=None, tokenizer='unsloth/meta-llama-3.1-8b-instruct-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=6000, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/meta-llama-3.1-8b-instruct-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":160}, use_cached_outputs=False, \n",
      "INFO 02-23 11:35:54 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 02-23 11:35:55 model_runner.py:1110] Starting to load model unsloth/meta-llama-3.1-8b-instruct-bnb-4bit...\n",
      "INFO 02-23 11:35:55 loader.py:1089] Loading weights with BitsAndBytes quantization.  May take a while ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W223 11:35:55.551062277 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-23 11:35:55 weight_utils.py:254] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15787d12e1b4700b5d29355f855440b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  64%|######4   | 3.66G/5.70G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-23 11:36:50 weight_utils.py:270] Time spent downloading weights for unsloth/meta-llama-3.1-8b-instruct-bnb-4bit: 54.171182 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1487bf4ad90f4c3aab6c1b898ca74aa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c22ad785b743fb9b9efcce064d45b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-23 11:37:42 model_runner.py:1115] Loading model weights took 5.3541 GB\n",
      "INFO 02-23 11:37:42 punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 02-23 11:37:47 worker.py:267] Memory profiling takes 4.56 seconds\n",
      "INFO 02-23 11:37:47 worker.py:267] the current vLLM instance can use total_gpu_memory (23.65GiB) x gpu_memory_utilization (0.41) = 9.73GiB\n",
      "INFO 02-23 11:37:47 worker.py:267] model weights take 5.35GiB; non_torch_memory takes 4.86GiB; PyTorch activation peak memory takes 0.78GiB; the rest of the memory reserved for KV Cache is -1.27GiB.\n",
      "INFO 02-23 11:37:47 executor_base.py:111] # cuda blocks: 0, # CPU blocks: 3072\n",
      "INFO 02-23 11:37:47 executor_base.py:116] Maximum concurrency for 6000 tokens per request: 0.00x\n",
      "Unsloth: Retrying vLLM to process 120 sequences and 26368 tokens in tandem.\n",
      "Error:\n",
      "No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine.\n",
      "INFO 02-23 11:37:49 config.py:549] This model supports multiple tasks: {'embed', 'generate', 'classify', 'score', 'reward'}. Defaulting to 'generate'.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection'], 'llm_int8_threshold': 6.0}\n",
      "INFO 02-23 11:37:49 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='unsloth/meta-llama-3.1-8b-instruct-bnb-4bit', speculative_config=None, tokenizer='unsloth/meta-llama-3.1-8b-instruct-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=6000, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/meta-llama-3.1-8b-instruct-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":120}, use_cached_outputs=False, \n",
      "INFO 02-23 11:37:50 model_runner.py:1110] Starting to load model unsloth/meta-llama-3.1-8b-instruct-bnb-4bit...\n",
      "INFO 02-23 11:37:50 loader.py:1089] Loading weights with BitsAndBytes quantization.  May take a while ...\n",
      "INFO 02-23 11:37:50 weight_utils.py:254] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be052dbd5a344e7b0fc0ed21645feb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2022b6dc70444d29efc763820946cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-23 11:37:53 model_runner.py:1115] Loading model weights took 5.3229 GB\n",
      "INFO 02-23 11:37:55 worker.py:267] Memory profiling takes 1.68 seconds\n",
      "INFO 02-23 11:37:55 worker.py:267] the current vLLM instance can use total_gpu_memory (23.65GiB) x gpu_memory_utilization (0.41) = 9.73GiB\n",
      "INFO 02-23 11:37:55 worker.py:267] model weights take 5.32GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 0.77GiB; the rest of the memory reserved for KV Cache is 3.63GiB.\n",
      "INFO 02-23 11:37:55 executor_base.py:111] # cuda blocks: 1859, # CPU blocks: 3072\n",
      "INFO 02-23 11:37:55 executor_base.py:116] Maximum concurrency for 6000 tokens per request: 4.96x\n",
      "INFO 02-23 11:37:57 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18/18 [00:13<00:00,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-23 11:38:11 model_runner.py:1562] Graph capturing finished in 14 secs, took -3.55 GiB\n",
      "INFO 02-23 11:38:11 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 18.64 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name, # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length = 6_000,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    "    fast_inference=True,\n",
    "    cache_dir = '/Data'\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt = '''\n",
    "    Voc√™ √© um assistente virtual que deve gerar resumos de textos em portugu√™s. \n",
    "    Seu resumo deve ter, no maximo 200 palavras e conter todas as informa√ß√µes principais do texto.\n",
    "    Esse √© o texto:\n",
    "\n",
    "    {text}\n",
    "\n",
    "    Fa√ßa um resumo de no maximo 200 palavras do texto acima\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "generated = []\n",
    "\n",
    "for _, row in tqdm(dataset.sort_values(\"num_tokens\", ascending= False).iterrows(), total = 5000):\n",
    "    prompt = base_prompt.format(text = row[\"text\"])\n",
    "    message = [{'role': 'user', 'content': prompt}]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        message,\n",
    "        tokenize = True,\n",
    "        add_generation_prompt = True, # Must add for generation\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    generated_ids = model.generate(inputs, max_new_tokens = 500)\n",
    "\n",
    "    generated_text = tokenizer.decode(generated_ids[0, inputs.shape[1]:])\n",
    "\n",
    "    generated.append({\n",
    "        'id': row['id'],\n",
    "        'prompt': prompt,\n",
    "        'generated_text': generated_text\n",
    "    })\n",
    "    # break\n",
    "    clear_output()\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>generated_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57237</td>\n",
       "      <td>\\n    Voc√™ √© um assistente virtual que deve ge...</td>\n",
       "      <td>O Los Angeles Lakers √© um time de basquetebol ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10306</td>\n",
       "      <td>\\n    Voc√™ √© um assistente virtual que deve ge...</td>\n",
       "      <td>O el√©tron √© uma part√≠cula subat√¥mica com carga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2118886</td>\n",
       "      <td>\\n    Voc√™ √© um assistente virtual que deve ge...</td>\n",
       "      <td>Sofia Doroteia Ulrica Alice, rainha consorte d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1695</td>\n",
       "      <td>\\n    Voc√™ √© um assistente virtual que deve ge...</td>\n",
       "      <td>Um sistema operacional √© um programa ou conjun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5448408</td>\n",
       "      <td>\\n    Voc√™ √© um assistente virtual que deve ge...</td>\n",
       "      <td>A crise migrat√≥ria venezuelana refere-se √† emi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1232</td>\n",
       "      <td>\\n    Voc√™ √© um assistente virtual que deve ge...</td>\n",
       "      <td>A monarquia √© uma forma de governo em que um m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12344</td>\n",
       "      <td>\\n    Voc√™ √© um assistente virtual que deve ge...</td>\n",
       "      <td>A Igreja Cat√≥lica √© a maior igreja crist√£ do m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>115343</td>\n",
       "      <td>\\n    Voc√™ √© um assistente virtual que deve ge...</td>\n",
       "      <td>O Athlon √© uma s√©rie de processadores da plata...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13675</td>\n",
       "      <td>\\n    Voc√™ √© um assistente virtual que deve ge...</td>\n",
       "      <td>Alfred Joseph Hitchcock foi um diretor e produ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7112</td>\n",
       "      <td>\\n    Voc√™ √© um assistente virtual que deve ge...</td>\n",
       "      <td>Tit√£ √© o maior sat√©lite natural de Saturno e o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1594</td>\n",
       "      <td>\\n    Voc√™ √© um assistente virtual que deve ge...</td>\n",
       "      <td>A qu√≠mica √© o estudo cient√≠fico das propriedad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5703</td>\n",
       "      <td>\\n    Voc√™ √© um assistente virtual que deve ge...</td>\n",
       "      <td>O zinco √© um elemento qu√≠mico de s√≠mbolo Zn, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>939</td>\n",
       "      <td>\\n    Voc√™ √© um assistente virtual que deve ge...</td>\n",
       "      <td>A Guin√©-Bissau √© um pa√≠s da √Åfrica Ocidental q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8695</td>\n",
       "      <td>\\n    Voc√™ √© um assistente virtual que deve ge...</td>\n",
       "      <td>O afric√¢ner, tamb√©m conhecido como afric√¢nder,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                             prompt  \\\n",
       "0     57237  \\n    Voc√™ √© um assistente virtual que deve ge...   \n",
       "1     10306  \\n    Voc√™ √© um assistente virtual que deve ge...   \n",
       "2   2118886  \\n    Voc√™ √© um assistente virtual que deve ge...   \n",
       "3      1695  \\n    Voc√™ √© um assistente virtual que deve ge...   \n",
       "4   5448408  \\n    Voc√™ √© um assistente virtual que deve ge...   \n",
       "5      1232  \\n    Voc√™ √© um assistente virtual que deve ge...   \n",
       "6     12344  \\n    Voc√™ √© um assistente virtual que deve ge...   \n",
       "7    115343  \\n    Voc√™ √© um assistente virtual que deve ge...   \n",
       "8     13675  \\n    Voc√™ √© um assistente virtual que deve ge...   \n",
       "9      7112  \\n    Voc√™ √© um assistente virtual que deve ge...   \n",
       "10     1594  \\n    Voc√™ √© um assistente virtual que deve ge...   \n",
       "11     5703  \\n    Voc√™ √© um assistente virtual que deve ge...   \n",
       "12      939  \\n    Voc√™ √© um assistente virtual que deve ge...   \n",
       "13     8695  \\n    Voc√™ √© um assistente virtual que deve ge...   \n",
       "\n",
       "                                       generated_text  \n",
       "0   O Los Angeles Lakers √© um time de basquetebol ...  \n",
       "1   O el√©tron √© uma part√≠cula subat√¥mica com carga...  \n",
       "2   Sofia Doroteia Ulrica Alice, rainha consorte d...  \n",
       "3   Um sistema operacional √© um programa ou conjun...  \n",
       "4   A crise migrat√≥ria venezuelana refere-se √† emi...  \n",
       "5   A monarquia √© uma forma de governo em que um m...  \n",
       "6   A Igreja Cat√≥lica √© a maior igreja crist√£ do m...  \n",
       "7   O Athlon √© uma s√©rie de processadores da plata...  \n",
       "8   Alfred Joseph Hitchcock foi um diretor e produ...  \n",
       "9   Tit√£ √© o maior sat√©lite natural de Saturno e o...  \n",
       "10  A qu√≠mica √© o estudo cient√≠fico das propriedad...  \n",
       "11  O zinco √© um elemento qu√≠mico de s√≠mbolo Zn, n...  \n",
       "12  A Guin√©-Bissau √© um pa√≠s da √Åfrica Ocidental q...  \n",
       "13  O afric√¢ner, tamb√©m conhecido como afric√¢nder,...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
