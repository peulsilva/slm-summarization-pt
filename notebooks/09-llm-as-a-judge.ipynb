{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar 12 13:33:04 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.124.04             Driver Version: 570.124.04     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A5000               Off |   00000000:01:00.0 Off |                  Off |\n",
      "| 30%   30C    P8             16W /  230W |    3146MiB /  24564MiB |      2%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A          426193      C   ...naconda3-2023.07-1/bin/python        268MiB |\n",
      "|    0   N/A  N/A          467312      C   ...naconda3-2023.07-1/bin/python        268MiB |\n",
      "|    0   N/A  N/A          974716      G   /usr/libexec/Xorg                        94MiB |\n",
      "|    0   N/A  N/A          974751      G   /usr/bin/gnome-shell                     19MiB |\n",
      "|    0   N/A  N/A         1477822      C   /usr/local/Anaconda3/bin/python        2438MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if 'notebooks' in os.getcwd():\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "from huggingface_hub import notebook_login\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import aiohttp\n",
    "import asyncio\n",
    "from tqdm.asyncio import tqdm\n",
    "import torch\n",
    "import scienceplots\n",
    "plt.style.use(['science', 'no-latex'])\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from src.text_utils import trim_text_to_token_limit\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from langdetect import detect\n",
    "\n",
    "from src.train_test_split import stratified_train_test_split\n",
    "\n",
    "import evaluate\n",
    "\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM as a Judge\n",
    "\n",
    "In this notebook, after generating all target summaries using Qwen 0.5B Instruct, Llama 1B Instruct, and Qwen 3B Instruct, we will evaluate the results using a larger Llama 8B Instruct model as an LLM judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\", cache_dir = '/Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_per_process_memory_fraction(0.6, device=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data = pd.read_json(\"data/wikipedia_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df , temp_df = stratified_train_test_split(base_data, test_size=0.4)\n",
    "val_df , test_df = stratified_train_test_split(temp_df, test_size=0.5)\n",
    "train_idx = train_df.id.tolist()\n",
    "val_idx = val_df.id.tolist()\n",
    "test_idx = test_df.id.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"data/generated_dataset_test_100_qwen-0.5b-instruct-summary-pt-rank{lora_rank}.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = []\n",
    "raw_model_df = pd.read_pickle(\"data/generated_dataset_Qwen2.5-0.5B-Instruct.pkl\")\n",
    "raw_model_df['model_name'] = \"Qwen-0.5B-Instruct\"\n",
    "\n",
    "all_df.append(raw_model_df)\n",
    "\n",
    "raw_model_df = pd.read_pickle(\"data/generated_dataset_test_100_Llama-3.2-1B-Instruct-bnb-4bit.pkl\")\n",
    "raw_model_df['model_name'] = \"Llama-1B-Instruct\"\n",
    "\n",
    "all_df.append(raw_model_df)\n",
    "\n",
    "raw_model_df = pd.read_pickle(\"data/generated_dataset_test_100_Qwen2.5-3B-Instruct-unsloth-bnb-4bit.pkl\")\n",
    "raw_model_df['model_name'] = \"Qwen-3B-Instruct\"\n",
    "\n",
    "all_df.append(raw_model_df)\n",
    "\n",
    "\n",
    "for lora_rank in [64]:\n",
    "    temp = pd.read_pickle(base_path.format(lora_rank = lora_rank))\n",
    "    \n",
    "    temp['model_name'] = \"Finetuned Model\"\n",
    "    all_df.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_summary = pd.read_pickle(\"data/generated_dataset_100_Meta-Llama-3.1-8B-Instruct-bnb-4bit_2.pkl\")\\\n",
    "    .rename(columns = {'generated_text': 'reference_summary'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.concat(all_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4629     1051\n",
       "691      1527\n",
       "901      1593\n",
       "1465      376\n",
       "48       1446\n",
       "         ... \n",
       "4079     1193\n",
       "2669      919\n",
       "18606     620\n",
       "12889     522\n",
       "45917     595\n",
       "Name: text, Length: 5000, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_data['text'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(\n",
    "    base_data[['id', 'text']],\n",
    "    temp_df,\n",
    "    on='id'\n",
    ").query(f\"id in {test_idx}\")\n",
    "\n",
    "df = pd.merge(df, reference_summary, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>generated_text</th>\n",
       "      <th>model_name</th>\n",
       "      <th>reference_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11194</td>\n",
       "      <td>O s√©culo XX iniciou em 1 de janeiro de 1901 e ...</td>\n",
       "      <td>O s√©culo XX foi marcado por uma s√©rie de avan√ß...</td>\n",
       "      <td>Qwen-0.5B-Instruct</td>\n",
       "      <td>O s√©culo XX foi marcado por grandes mudan√ßas t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11194</td>\n",
       "      <td>O s√©culo XX iniciou em 1 de janeiro de 1901 e ...</td>\n",
       "      <td>O s√©culo XX foi marcado por avan√ßos tecnol√≥gic...</td>\n",
       "      <td>Llama-1B-Instruct</td>\n",
       "      <td>O s√©culo XX foi marcado por grandes mudan√ßas t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11194</td>\n",
       "      <td>O s√©culo XX iniciou em 1 de janeiro de 1901 e ...</td>\n",
       "      <td>O s√©culo XX foi marcado por avan√ßos tecnol√≥gic...</td>\n",
       "      <td>Qwen-3B-Instruct</td>\n",
       "      <td>O s√©culo XX foi marcado por grandes mudan√ßas t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11194</td>\n",
       "      <td>O s√©culo XX iniciou em 1 de janeiro de 1901 e ...</td>\n",
       "      <td>O s√©culo XX foi um per√≠odo de in√∫meros avan√ßos...</td>\n",
       "      <td>Finetuned Model</td>\n",
       "      <td>O s√©culo XX foi marcado por grandes mudan√ßas t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4771</td>\n",
       "      <td>Chile (; ; ), oficialmente Rep√∫blica do Chile ...</td>\n",
       "      <td>Aqui est√° um resumo do texto:\\n\\nChile (; ; ),...</td>\n",
       "      <td>Qwen-0.5B-Instruct</td>\n",
       "      <td>O Chile √© um pa√≠s da Am√©rica do Sul que ocupa ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>3944</td>\n",
       "      <td>O Sud√£o (; ), oficialmente Rep√∫blica do Sud√£o ...</td>\n",
       "      <td>O Sud√£o √© um pa√≠s africano, limitado por Egito...</td>\n",
       "      <td>Finetuned Model</td>\n",
       "      <td>O Sud√£o √© um pa√≠s africano localizado no norte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>165327</td>\n",
       "      <td>XviD √© um software livre e codec de v√≠deo MPEG...</td>\n",
       "      <td>XviD √© um software livre e codec de v√≠deo MPEG...</td>\n",
       "      <td>Qwen-0.5B-Instruct</td>\n",
       "      <td>XviD √© um software livre e codec de v√≠deo MPEG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>165327</td>\n",
       "      <td>XviD √© um software livre e codec de v√≠deo MPEG...</td>\n",
       "      <td>XviD √© um software livre e codec de v√≠deo MPEG...</td>\n",
       "      <td>Llama-1B-Instruct</td>\n",
       "      <td>XviD √© um software livre e codec de v√≠deo MPEG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>165327</td>\n",
       "      <td>XviD √© um software livre e codec de v√≠deo MPEG...</td>\n",
       "      <td>XviD √© um codec de v√≠deo c√≥digo aberto que com...</td>\n",
       "      <td>Qwen-3B-Instruct</td>\n",
       "      <td>XviD √© um software livre e codec de v√≠deo MPEG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>165327</td>\n",
       "      <td>XviD √© um software livre e codec de v√≠deo MPEG...</td>\n",
       "      <td>XviD √© um software livre e codec de v√≠deo MPEG...</td>\n",
       "      <td>Finetuned Model</td>\n",
       "      <td>XviD √© um software livre e codec de v√≠deo MPEG...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                               text  \\\n",
       "0      11194  O s√©culo XX iniciou em 1 de janeiro de 1901 e ...   \n",
       "1      11194  O s√©culo XX iniciou em 1 de janeiro de 1901 e ...   \n",
       "2      11194  O s√©culo XX iniciou em 1 de janeiro de 1901 e ...   \n",
       "3      11194  O s√©culo XX iniciou em 1 de janeiro de 1901 e ...   \n",
       "4       4771  Chile (; ; ), oficialmente Rep√∫blica do Chile ...   \n",
       "...      ...                                                ...   \n",
       "3995    3944  O Sud√£o (; ), oficialmente Rep√∫blica do Sud√£o ...   \n",
       "3996  165327  XviD √© um software livre e codec de v√≠deo MPEG...   \n",
       "3997  165327  XviD √© um software livre e codec de v√≠deo MPEG...   \n",
       "3998  165327  XviD √© um software livre e codec de v√≠deo MPEG...   \n",
       "3999  165327  XviD √© um software livre e codec de v√≠deo MPEG...   \n",
       "\n",
       "                                         generated_text          model_name  \\\n",
       "0     O s√©culo XX foi marcado por uma s√©rie de avan√ß...  Qwen-0.5B-Instruct   \n",
       "1     O s√©culo XX foi marcado por avan√ßos tecnol√≥gic...   Llama-1B-Instruct   \n",
       "2     O s√©culo XX foi marcado por avan√ßos tecnol√≥gic...    Qwen-3B-Instruct   \n",
       "3     O s√©culo XX foi um per√≠odo de in√∫meros avan√ßos...     Finetuned Model   \n",
       "4     Aqui est√° um resumo do texto:\\n\\nChile (; ; ),...  Qwen-0.5B-Instruct   \n",
       "...                                                 ...                 ...   \n",
       "3995  O Sud√£o √© um pa√≠s africano, limitado por Egito...     Finetuned Model   \n",
       "3996  XviD √© um software livre e codec de v√≠deo MPEG...  Qwen-0.5B-Instruct   \n",
       "3997  XviD √© um software livre e codec de v√≠deo MPEG...   Llama-1B-Instruct   \n",
       "3998  XviD √© um codec de v√≠deo c√≥digo aberto que com...    Qwen-3B-Instruct   \n",
       "3999  XviD √© um software livre e codec de v√≠deo MPEG...     Finetuned Model   \n",
       "\n",
       "                                      reference_summary  \n",
       "0     O s√©culo XX foi marcado por grandes mudan√ßas t...  \n",
       "1     O s√©culo XX foi marcado por grandes mudan√ßas t...  \n",
       "2     O s√©culo XX foi marcado por grandes mudan√ßas t...  \n",
       "3     O s√©culo XX foi marcado por grandes mudan√ßas t...  \n",
       "4     O Chile √© um pa√≠s da Am√©rica do Sul que ocupa ...  \n",
       "...                                                 ...  \n",
       "3995  O Sud√£o √© um pa√≠s africano localizado no norte...  \n",
       "3996  XviD √© um software livre e codec de v√≠deo MPEG...  \n",
       "3997  XviD √© um software livre e codec de v√≠deo MPEG...  \n",
       "3998  XviD √© um software livre e codec de v√≠deo MPEG...  \n",
       "3999  XviD √© um software livre e codec de v√≠deo MPEG...  \n",
       "\n",
       "[4000 rows x 5 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-12 09:21:30 __init__.py:207] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A5000. Max memory: 23.536 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/meta-llama-3.1-8b-instruct-bnb-4bit with actual GPU utilization = 48.12%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.6 with VRAM = 23.54 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 6000. Num Sequences = 192.\n",
      "Unsloth: vLLM's KV Cache can use up to 4.99 GB. Also swap space = 6 GB.\n",
      "INFO 03-12 09:21:42 config.py:549] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection'], 'llm_int8_threshold': 6.0}\n",
      "INFO 03-12 09:21:43 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='unsloth/meta-llama-3.1-8b-instruct-bnb-4bit', speculative_config=None, tokenizer='unsloth/meta-llama-3.1-8b-instruct-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=6000, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/meta-llama-3.1-8b-instruct-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":192}, use_cached_outputs=False, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36817a48b0de44508c119f0cb5d43d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b487b798a934f28b73f153c602d9300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43f82934eccc4bd9a219142061ef9cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e152cd0e7b424f82b0561d5dd3befcf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-12 09:21:46 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 03-12 09:21:46 model_runner.py:1110] Starting to load model unsloth/meta-llama-3.1-8b-instruct-bnb-4bit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W312 09:21:46.685432046 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-12 09:21:46 loader.py:1089] Loading weights with BitsAndBytes quantization.  May take a while ...\n",
      "INFO 03-12 09:21:47 weight_utils.py:254] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "847c79f0699e41a988cedfbcafe3f3e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-12 09:23:16 weight_utils.py:270] Time spent downloading weights for unsloth/meta-llama-3.1-8b-instruct-bnb-4bit: 88.939799 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "608cd2a8a0b14500b8cfcb2fab79a269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37c9776193a647fcb7fc49801a25288a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-12 09:24:07 model_runner.py:1115] Loading model weights took 5.3541 GB\n",
      "INFO 03-12 09:24:07 punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 03-12 09:24:10 worker.py:267] Memory profiling takes 3.37 seconds\n",
      "INFO 03-12 09:24:10 worker.py:267] the current vLLM instance can use total_gpu_memory (23.54GiB) x gpu_memory_utilization (0.48) = 11.33GiB\n",
      "INFO 03-12 09:24:10 worker.py:267] model weights take 5.35GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 0.93GiB; the rest of the memory reserved for KV Cache is 4.99GiB.\n",
      "INFO 03-12 09:24:11 executor_base.py:111] # cuda blocks: 2554, # CPU blocks: 3072\n",
      "INFO 03-12 09:24:11 executor_base.py:116] Maximum concurrency for 6000 tokens per request: 6.81x\n",
      "INFO 03-12 09:24:13 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:20<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-12 09:24:33 model_runner.py:1562] Graph capturing finished in 20 secs, took 0.68 GiB\n",
      "INFO 03-12 09:24:33 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 25.90 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name, # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length = 6_000,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    "    fast_inference=True,\n",
    "    cache_dir = '/Data'\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt = '''\n",
    "    Voc√™ √© um assistente √∫til que classifica resumos.  \n",
    "    Fornecerei o texto e dois resumos (0 e 1) de aproximadamente 100 palavras desse texto em portugu√™s.  \n",
    "\n",
    "    Voc√™ deve indicar qual deles √© o melhor resumo, com base tanto na qualidade do resumo quanto na qualidade do texto em portugu√™s e no tamanho do texto (deve ter aproximadamente 100 palavras).\n",
    "\n",
    "    Aqui est√° o texto:  \n",
    "    <text>  \n",
    "    {text}  \n",
    "    </text>  \n",
    "\n",
    "    Aqui est√° o resumo 0:  \n",
    "    <0>  \n",
    "    {summary_0}  \n",
    "    </0>  \n",
    "\n",
    "    Aqui est√° o resumo 1:  \n",
    "    <1>  \n",
    "    {summary_1}  \n",
    "    </1>  \n",
    "\n",
    "    Responda no seguinte formato (JSON):  \n",
    "    \n",
    "    {{\n",
    "        \"best_summary\": (0 ou 1),\n",
    "        \"explanation\": \"uma breve explica√ß√£o do porqu√™.\"\n",
    "    }}\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 34/1000 [06:42<3:10:23, 11.83s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 48\u001b[0m\n\u001b[1;32m     40\u001b[0m message \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m: prompt}]\n\u001b[1;32m     41\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(\n\u001b[1;32m     42\u001b[0m     message,\n\u001b[1;32m     43\u001b[0m     tokenize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     44\u001b[0m     add_generation_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;66;03m# Must add for generation\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     return_tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     46\u001b[0m )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(inputs, max_new_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m)\n\u001b[1;32m     50\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(generated_ids[\u001b[38;5;241m0\u001b[39m, inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:])\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|eot_id|>\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(index_of_finetune)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/unsloth/models/llama.py:1596\u001b[0m, in \u001b[0;36m_wrap_fast_inference.<locals>._fast_generate\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m \u001b[38;5;66;03m# Set pad token\u001b[39;00m\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;66;03m# old_pad_token_id = getattr(model.config, \"pad_token_id\", None)\u001b[39;00m\n\u001b[1;32m   1591\u001b[0m \u001b[38;5;66;03m# old_eos_token_id = getattr(model.config, \"eos_token_id\", None)\u001b[39;00m\n\u001b[1;32m   1592\u001b[0m \u001b[38;5;66;03m# model.config.pad_token_id = old_eos_token_id\u001b[39;00m\n\u001b[1;32m   1593\u001b[0m \n\u001b[1;32m   1594\u001b[0m \u001b[38;5;66;03m# Autocasted\u001b[39;00m\n\u001b[1;32m   1595\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type \u001b[38;5;241m=\u001b[39m device_type, dtype \u001b[38;5;241m=\u001b[39m dtype):\n\u001b[0;32m-> 1596\u001b[0m     output \u001b[38;5;241m=\u001b[39m generate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m \u001b[38;5;66;03m# Revert\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m \u001b[38;5;66;03m# model.config.pad_token_id = old_pad_token_id\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m \n\u001b[1;32m   1602\u001b[0m \u001b[38;5;66;03m# Unset a flag for generation!\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/generation/utils.py:2223\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2215\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2216\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2217\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2218\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2219\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2220\u001b[0m     )\n\u001b[1;32m   2222\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2223\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[1;32m   2224\u001b[0m         input_ids,\n\u001b[1;32m   2225\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[1;32m   2226\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[1;32m   2227\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[1;32m   2228\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[1;32m   2229\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[1;32m   2230\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2231\u001b[0m     )\n\u001b[1;32m   2233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2236\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2237\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2242\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2243\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/generation/utils.py:3214\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3212\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3214\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   3216\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3217\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3218\u001b[0m     outputs,\n\u001b[1;32m   3219\u001b[0m     model_kwargs,\n\u001b[1;32m   3220\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3221\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/unsloth/models/llama.py:1043\u001b[0m, in \u001b[0;36mCausalLM_fast_forward.<locals>._CausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, logits_to_keep, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_CausalLM_fast_forward\u001b[39m(\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1026\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1040\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple, CausalLMOutputWithPast]:\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1043\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m fast_forward_inference(\n\u001b[1;32m   1044\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1045\u001b[0m             input_ids,\n\u001b[1;32m   1046\u001b[0m             past_key_values,\n\u001b[1;32m   1047\u001b[0m             position_ids \u001b[38;5;241m=\u001b[39m position_ids,\n\u001b[1;32m   1048\u001b[0m             attention_mask \u001b[38;5;241m=\u001b[39m attention_mask,\n\u001b[1;32m   1049\u001b[0m         )\n\u001b[1;32m   1050\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1051\u001b[0m         causal_mask \u001b[38;5;241m=\u001b[39m xformers\u001b[38;5;241m.\u001b[39mattn_bias\u001b[38;5;241m.\u001b[39mLowerTriangularMask() \u001b[38;5;28;01mif\u001b[39;00m HAS_XFORMERS \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/unsloth/models/llama.py:978\u001b[0m, in \u001b[0;36mLlamaModel_fast_forward_inference\u001b[0;34m(self, input_ids, past_key_values, position_ids, attention_mask)\u001b[0m\n\u001b[1;32m    970\u001b[0m residual\u001b[38;5;241m.\u001b[39mcopy_(X) \u001b[38;5;66;03m# residual = X\u001b[39;00m\n\u001b[1;32m    971\u001b[0m X \u001b[38;5;241m=\u001b[39m fast_rms_layernorm_inference(\n\u001b[1;32m    972\u001b[0m     decoder_layer\u001b[38;5;241m.\u001b[39minput_layernorm,\n\u001b[1;32m    973\u001b[0m     X,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    976\u001b[0m     variance \u001b[38;5;241m=\u001b[39m variance,\n\u001b[1;32m    977\u001b[0m )\n\u001b[0;32m--> 978\u001b[0m X, present_key_value \u001b[38;5;241m=\u001b[39m LlamaAttention_fast_forward_inference(\n\u001b[1;32m    979\u001b[0m     decoder_layer\u001b[38;5;241m.\u001b[39mself_attn,\n\u001b[1;32m    980\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m X,\n\u001b[1;32m    981\u001b[0m     past_key_value \u001b[38;5;241m=\u001b[39m past_key_values[idx],\n\u001b[1;32m    982\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m position_ids,\n\u001b[1;32m    983\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m attention_mask,\n\u001b[1;32m    984\u001b[0m     do_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(decoder_layer\u001b[38;5;241m.\u001b[39mself_attn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaged_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    985\u001b[0m )\n\u001b[1;32m    986\u001b[0m X \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m residual\n\u001b[1;32m    988\u001b[0m residual\u001b[38;5;241m.\u001b[39mcopy_(X) \u001b[38;5;66;03m# residual = X\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/unsloth/models/llama.py:218\u001b[0m, in \u001b[0;36mLlamaAttention_fast_forward_inference\u001b[0;34m(self, hidden_states, past_key_value, position_ids, do_prefill, attention_mask)\u001b[0m\n\u001b[1;32m    216\u001b[0m RH_Q[:,:,:,:h] \u001b[38;5;241m=\u001b[39m Qn[:,:,:,h:]\n\u001b[1;32m    217\u001b[0m RH_Q[:,:,:,h:] \u001b[38;5;241m=\u001b[39m Qn[:,:,:,:h]\n\u001b[0;32m--> 218\u001b[0m torch\u001b[38;5;241m.\u001b[39mneg(RH_Q[:,:,:,:h], out \u001b[38;5;241m=\u001b[39m RH_Q[:,:,:,:h])\n\u001b[1;32m    219\u001b[0m Qn \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m cos\n\u001b[1;32m    220\u001b[0m Qn\u001b[38;5;241m.\u001b[39maddcmul_(RH_Q, sin)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "generated = []\n",
    "\n",
    "for _, group in tqdm(df.groupby(\"id\"), total = len(df)//4):\n",
    "\n",
    "    finetuned_summary = group.query(\"model_name == 'Finetuned Model'\")\n",
    "\n",
    "    text_finetune = finetuned_summary['generated_text'].item()\n",
    "\n",
    "    for idx, row in group.iterrows():\n",
    "        if row['model_name'] == 'Finetuned Model':\n",
    "            continue\n",
    "        text_model = row['generated_text']\n",
    "\n",
    "\n",
    "        index_of_finetune = int(np.random.random() > 1/2)\n",
    "    \n",
    "        shuffling_dict = {\n",
    "            index_of_finetune: text_finetune,\n",
    "            1 - index_of_finetune: text_model\n",
    "        }\n",
    "\n",
    "        inverse_shuffling_map = {\n",
    "            index_of_finetune: \"Finetuned Model\",\n",
    "            1 - index_of_finetune: row['model_name']\n",
    "        }\n",
    "\n",
    "        prompt = base_prompt.format(\n",
    "            text = finetuned_summary[\"text\"].item(),\n",
    "            summary_0 = shuffling_dict[0],\n",
    "            summary_1 = shuffling_dict[1]\n",
    "        )\n",
    "\n",
    "        message = [{'role': 'user', 'content': prompt}]\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            message,\n",
    "            tokenize = True,\n",
    "            add_generation_prompt = True, # Must add for generation\n",
    "            return_tensors = \"pt\",\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        generated_ids = model.generate(inputs, max_new_tokens = 500)\n",
    "\n",
    "        generated_text = tokenizer.decode(generated_ids[0, inputs.shape[1]:]).split(\"<|eot_id|>\")[0]\n",
    "\n",
    "        print(index_of_finetune)\n",
    "        print(row['model_name'])\n",
    "        print(generated_text)\n",
    "\n",
    "        try:\n",
    "            generated_json = json.loads(generated_text)\n",
    "            generated_json['best_summary'] = inverse_shuffling_map[generated_json['best_summary']]\n",
    "            generated_json[\"index_of_modified\"] = index_of_finetune\n",
    "\n",
    "            new_row = {\n",
    "                'id': row['id'],\n",
    "                'model': row['model_name'],\n",
    "                'winner': generated_json['best_summary'],\n",
    "                'explanation': generated_json['explanation']\n",
    "            }\n",
    "\n",
    "            print(new_row)\n",
    "\n",
    "            generated.append(new_row)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing LLM: {e}\")\n",
    "\n",
    "\n",
    "        \n",
    "    # break\n",
    "    clear_output()\n",
    "    # print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model</th>\n",
       "      <th>winner</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>228</td>\n",
       "      <td>Qwen-0.5B-Instruct</td>\n",
       "      <td>Finetuned Model</td>\n",
       "      <td>O resumo 0 fornece uma vis√£o mais clara e conc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>228</td>\n",
       "      <td>Llama-1B-Instruct</td>\n",
       "      <td>Finetuned Model</td>\n",
       "      <td>O resumo 1 √© mais preciso e detalhado, abordan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>228</td>\n",
       "      <td>Qwen-3B-Instruct</td>\n",
       "      <td>Finetuned Model</td>\n",
       "      <td>O resumo 1 fornece uma vis√£o mais completa e p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>230</td>\n",
       "      <td>Qwen-0.5B-Instruct</td>\n",
       "      <td>Qwen-0.5B-Instruct</td>\n",
       "      <td>O resumo 1 √© mais completo e preciso, abordand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>230</td>\n",
       "      <td>Llama-1B-Instruct</td>\n",
       "      <td>Llama-1B-Instruct</td>\n",
       "      <td>O resumo 1 √© mais completo e preciso, abordand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>745</td>\n",
       "      <td>Llama-1B-Instruct</td>\n",
       "      <td>Finetuned Model</td>\n",
       "      <td>O resumo 1 √© mais completo e preciso, abordand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>745</td>\n",
       "      <td>Qwen-3B-Instruct</td>\n",
       "      <td>Qwen-3B-Instruct</td>\n",
       "      <td>O resumo 1 √© mais preciso e conciso, capturand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>752</td>\n",
       "      <td>Qwen-0.5B-Instruct</td>\n",
       "      <td>Qwen-0.5B-Instruct</td>\n",
       "      <td>O resumo 0 √© mais preciso e completo, abordand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>752</td>\n",
       "      <td>Llama-1B-Instruct</td>\n",
       "      <td>Llama-1B-Instruct</td>\n",
       "      <td>O resumo 1 √© mais conciso e eficaz em capturar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>752</td>\n",
       "      <td>Qwen-3B-Instruct</td>\n",
       "      <td>Qwen-3B-Instruct</td>\n",
       "      <td>O resumo 1 √© mais conciso e eficaz em capturar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id               model              winner  \\\n",
       "0    228  Qwen-0.5B-Instruct     Finetuned Model   \n",
       "1    228   Llama-1B-Instruct     Finetuned Model   \n",
       "2    228    Qwen-3B-Instruct     Finetuned Model   \n",
       "3    230  Qwen-0.5B-Instruct  Qwen-0.5B-Instruct   \n",
       "4    230   Llama-1B-Instruct   Llama-1B-Instruct   \n",
       "..   ...                 ...                 ...   \n",
       "97   745   Llama-1B-Instruct     Finetuned Model   \n",
       "98   745    Qwen-3B-Instruct    Qwen-3B-Instruct   \n",
       "99   752  Qwen-0.5B-Instruct  Qwen-0.5B-Instruct   \n",
       "100  752   Llama-1B-Instruct   Llama-1B-Instruct   \n",
       "101  752    Qwen-3B-Instruct    Qwen-3B-Instruct   \n",
       "\n",
       "                                           explanation  \n",
       "0    O resumo 0 fornece uma vis√£o mais clara e conc...  \n",
       "1    O resumo 1 √© mais preciso e detalhado, abordan...  \n",
       "2    O resumo 1 fornece uma vis√£o mais completa e p...  \n",
       "3    O resumo 1 √© mais completo e preciso, abordand...  \n",
       "4    O resumo 1 √© mais completo e preciso, abordand...  \n",
       "..                                                 ...  \n",
       "97   O resumo 1 √© mais completo e preciso, abordand...  \n",
       "98   O resumo 1 √© mais preciso e conciso, capturand...  \n",
       "99   O resumo 0 √© mais preciso e completo, abordand...  \n",
       "100  O resumo 1 √© mais conciso e eficaz em capturar...  \n",
       "101  O resumo 1 √© mais conciso e eficaz em capturar...  \n",
       "\n",
       "[102 rows x 4 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model\n",
       "Llama-1B-Instruct     0.511202\n",
       "Qwen-0.5B-Instruct    0.607254\n",
       "Qwen-3B-Instruct      0.222680\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_winner(x):\n",
    "    return (x['winner'] == \"Finetuned Model\").mean()\n",
    "pd.read_pickle(\"data/win_rates.pkl\").groupby(\"model\").apply(get_winner)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
